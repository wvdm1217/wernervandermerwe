---
title: 'A Tale of a Raggedy Bot' 
date: 2024-01-07
url: 'qa-bot'
layout: 'doc'
---

I built a raggedy question and answer bot.
I have shared my code if you want to run it yourself [here](insert).
It is my attempt at demonstrating how a generalist large language model (LLM)might become a specialist if given more relevant context.
You will need an Open AI API key or modify the code to use another model.

## A Generalist

```
The cure for boredom is curiosity. 
There is no cure for curiosity.
- Dorothy Parker
```

I think a strong argument for the cure for curiosity can be made by the wide adoption of LLMs. 
We have an expert and wise teacher available to answer our burning questions every second of the day.
It might not cure curiosity, but it can keep up with most of your relentless queries.

However, this teacher or companion is a generalist.
The quality of answer to a question is dependent on the models training data and the context that is provided as part of the question.
LLMs start hallucinating if it does not have enough context to answer the question.
Your wise teacher then becomes a misleading teacher.
It prefers winding together a well knit lie, a [hallucination](https://www.promptingguide.ai/risks/factuality), instead of confessing that it does not know the answer.

How might we provide the context needed to answer a question that is domain specific?
As an example, a company has internal documentation for new employees, how can they get a bot to incorporate this information when assisting new employees?
They could fine-tune an open source model, i.e. train it further on the entirety of the new dataset.
This can be very technical and not always feasible.
The alternative is to selectively incorporate pieces of information from the new dataset as context in the prompt itself.


## Retrieval Augmented Generation (RAG)

Retrieval augmented generation (RAG) is a technique used to add context to LLM prompts.
This context is retrieved from a domain specific knowledge base. 
RAG is one of the techniques that is being used to try and increase the performance of LLMs on domain specific tasks. 
Including important pieces of context in a prompt leads to vastly improved responses. 



[RAG](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) was first proposed by [Meta](https://ai.meta.com) back in 2020.

## RAGgedy Bot

## 

## References

1. https://www.promptingguide.ai/techniques/rag
2. https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/